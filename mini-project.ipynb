{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Review & Cleaning\n",
    "\n",
    "#### 1.1. Initial review - remove redundant columns, standardize column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import squarify  # to draw treemap in matplotlib\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.special import inv_boxcox       # to use absolute value\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "data = pd.read_csv(\"fifa21_male2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a quick look at the data:\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice immediately that the columns `Player Photo`, `Club Logo`, and `Flag Photo` contain *sofifa* links, which will not help in the current analysis, so we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Player Photo', 'Club Logo', 'Flag Photo'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice there is an ID column which we can use as our index column, after ensuring it doesn't have any duplicate values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data['ID'].unique())/data.shape[0]) # equal to 1, so no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('ID', inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also remove the `Name` column, as it's unlikely to be relevant to our analysis and store it in a different dataframe for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = data['Name']\n",
    "# data.drop('Name', axis=1, inplace=True) # We will remove it after EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice there are many columns names which are abbreviations - so we will rename those to the full name for clarity. To do so, we are using a `.csv` file with the abbreviations and their corresponding meaning, as seen on the *sofifa* website. Alongside, we will also standardize the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change capitalization to lowercase and replace spaces with underscores:\n",
    "data.columns = data.columns.str.lower()\n",
    "data.columns = data.columns.str.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read positions.csv into dataframe\n",
    "positions = pd.read_csv('positions.csv', header=None, index_col=0)\n",
    "\n",
    "# Change dataframe to series and then dictionary so it can be used to rename columns:\n",
    "positions = positions.squeeze().to_dict()\n",
    "\n",
    "# Change column names:\n",
    "for column in data.columns:\n",
    "    if column in positions:\n",
    "        data.rename(columns=positions, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can quickly check which columns have more than 75% null values, so we can discard them from our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_values(df, threshold=75):\n",
    "    nulls_percentage = {}\n",
    "    for column in df.columns:\n",
    "        number_of_nulls = df[column].isna().sum()\n",
    "        null_percentage = round(number_of_nulls * 100 / df.shape[0], 1)\n",
    "        if null_percentage >= threshold:\n",
    "            nulls_percentage[column] = null_percentage\n",
    "    return nulls_percentage\n",
    "\n",
    "check_null_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As `loan_date_end` has mostly `NaN` values, we can discard it:\n",
    "data.drop('loan_date_end', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of the amount of null values in our data, we can check the maximum percentage of `NaN` values in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_nulls(df):\n",
    "    nulls_percentage = []\n",
    "    for column in df.columns:\n",
    "        number_of_nulls = df[column].isna().sum()\n",
    "        null_percentage = round(number_of_nulls * 100 / df.shape[0], 1)\n",
    "        nulls_percentage.append(null_percentage)\n",
    "    return max(nulls_percentage)\n",
    "\n",
    "max_nulls(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the maximum amount of nulls is low (2.5%), we can continue with our initial review and get back to replacing null values later. \n",
    "\n",
    "Next, we'll review the number of unique values per column and ensure there are no columns with one value only, as they will not add any value to the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_values(df):\n",
    "    single_value_columns = []\n",
    "    for column in df:\n",
    "        if len(df[column].unique()) == 1:\n",
    "            single_value_columns.append(column)\n",
    "    return single_value_columns\n",
    "\n",
    "check_unique_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values in the gender column\n",
    "data['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the gender column as the data shows only male players\n",
    "data.drop('gender', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking once more at the data, we observe that the `team_&_contract` column seems to have similar information to the `club` & `contract` columns, so we might be able to remove it too. However, we first have to check our assumption is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a joined column to compare to the team & contract column\n",
    "for i in range(0, data.shape[0]):\n",
    "    if data['contract'].iloc[i] == np.nan:\n",
    "        data['contract'].iloc[i] = ' '\n",
    "    if data['club'].iloc[i] == np.nan:\n",
    "        data['club'].iloc[i] = ' '\n",
    "\n",
    "data['club_&_contract'] = data['club'] + ' ' + data['contract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if columns are identical\n",
    "data['club_&_contract'].equals(data['team_&_contract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the columns are not identical, we'll extract the non-identical values to a dataframe to check them\n",
    "def check_identical_columns(col1, col2, df):\n",
    "    diff_values = pd.DataFrame({col1:[], col2:[]})\n",
    "\n",
    "    for i in range(0, df.shape[0]):\n",
    "        if df[col1].iloc[i] == df[col2].iloc[i]:\n",
    "            continue\n",
    "        else:\n",
    "            diff_values.loc[len(diff_values.index)] = [df[col1].iloc[i], df[col2].iloc[i]]\n",
    "\n",
    "    return diff_values       \n",
    "            \n",
    "diff_values = check_identical_columns(col1='club_&_contract', col2='team_&_contract', df=data)        \n",
    "\n",
    "# diff_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice two things:\n",
    "- the `club_&_contract` column has null values, whereas the `team_&_contract` column doesn't\n",
    "* the `club_&_contract` rows that are non-identical also contain the country that the club belongs to\n",
    "\n",
    "As the number of values containing the country are low compared to the number of rows in the dataframe, we'll quickly check if the null values come from the `club` and / or `contract` column to see if we can discard the `team_&_contract` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the rows with null values\n",
    "diff_values[diff_values['club_&_contract'].isna()]\n",
    "\n",
    "# Check if the nulls come from the contract or club columns:\n",
    "print(data['contract'].isna().sum())\n",
    "print(data['club'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the null values in the `club_&_contract` column came from the `club` column. However, given that the `team_&_contract` values corresponding to those rows also contain only the contract information, we can remove it, as it doesn't provide any additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['team_&_contract', 'club_&_contract'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Data Cleaning\n",
    "\n",
    "##### 1.2.1. Numerical Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`value`, `wage`, and `release clause` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financials = ['value', 'wage', 'release_clause']\n",
    "\n",
    "def clean_value(i):\n",
    "    x = float(i.replace(\".\",\"\").replace(\"€\",\"\").replace(\"K\",\"000\").replace(\"M\",\"00000\"))\n",
    "    return x\n",
    "\n",
    "for column in financials:\n",
    "    data[column] = data[column].apply(clean_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weight` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_weight(i):\n",
    "    x = float(i.replace('lbs',''))\n",
    "    return x\n",
    "\n",
    "data[\"weight\"] = data[\"weight\"].apply(clean_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`height` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_height(i):\n",
    "    to_cm = 2.54\n",
    "    x = i.replace(\"'\",\" \")\n",
    "    x2 = x.replace('\"','') # need to do it in two parts \n",
    "                           # because of different quote used \n",
    "                           # for inch and foot \n",
    "    y = x2.split()\n",
    "    height = round(((float(y[0])*12)+float(y[1]))*to_cm,0)\n",
    "    return height\n",
    "\n",
    "data['height'] = data['height'].apply(convert_height)\n",
    "# data['height']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the positions columns, i.e. `left-striker`, `goalkeeper`, etc. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_positions(i):\n",
    "    x = float(i.replace(\"+\",\".\").replace(\"-\",\"\"))\n",
    "    return x\n",
    "\n",
    "for col in data.loc[:, 'left_striker':'goalkeeper']:\n",
    "    data[col] = data[col].apply(cleaning_positions)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to cleaning the data within, we will also find which columns have identical scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll first extract the names of the positions that have players on the right / left / center\n",
    "first_position = data.columns.get_loc('left_striker') \n",
    "last_position = data.columns.get_loc('goalkeeper')\n",
    "\n",
    "position_names = []\n",
    "\n",
    "for i in range(first_position, last_position + 1):\n",
    "    column_name = data.columns[i]\n",
    "    if 'left' in column_name:\n",
    "        position_names.append(column_name[5:])\n",
    "\n",
    "print(position_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll find the groups that we'd like to check\n",
    "groups = {}\n",
    "\n",
    "for position in position_names:\n",
    "    placements = []\n",
    "    for i in range(first_position, last_position + 1):\n",
    "        column_name = data.columns[i]\n",
    "        if position in column_name:\n",
    "            placements.append(column_name)\n",
    "    groups[position] = placements\n",
    "\n",
    "# Remove the extra values from the 'back' list:\n",
    "groups['back'] = ['left_back', 'center_back', 'right_back']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find identical columns and store the indexes in a list to later drop\n",
    "to_drop = []\n",
    "\n",
    "for position in groups.keys():\n",
    "    positions_number = len(groups[position])\n",
    "    for i in range(0, positions_number):\n",
    "        for j in range(i + 1, positions_number):\n",
    "            column_1 = groups[position][i]\n",
    "            column_2 = groups[position][j]\n",
    "            if data[column_1].equals(data[column_2]):\n",
    "                if column_2 not in to_drop:\n",
    "                    to_drop.append(column_2)\n",
    "\n",
    "# Drop redundant columns\n",
    "data.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'left_' string from the remaining columns\n",
    "for column in data.columns:\n",
    "    if 'left' in column:\n",
    "        data.rename({column: column[5:]}, axis=1, inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weak_foot`, `skill_moves`, and `international_reputation` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_columns = ['weak_foot', 'skill_moves', 'international_reputation']\n",
    "\n",
    "# Check unique values\n",
    "for column in star_columns:\n",
    "    print(data[column].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the first character from the string, then convert the data type to integer\n",
    "# and check the operation was successful\n",
    "for column in star_columns:\n",
    "    data[column] = data[column].str[0]\n",
    "    data[column] = pd.to_numeric(data[column], errors='raise')\n",
    "    print(data[column].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2. Categorical Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the `positions` data to the number of possible positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_positions(x):\n",
    "    number_of_positions = str(x).split() \n",
    "    position = len(number_of_positions) \n",
    "    return position \n",
    "\n",
    "data['position'] = data['position'].apply(clean_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the year from the `joined` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the end of the joined is typically represented by the last 4 characters of the \n",
    "# strings, we will extract those where possible:\n",
    "def extract_year(x):\n",
    "    try:\n",
    "        x = int(x[-4:])\n",
    "    except:\n",
    "        pass\n",
    "    return x\n",
    "\n",
    "data['joined'] = data['joined'].apply(extract_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same function to extract the end year from the `contract` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the contract data\n",
    "data['contract'].unique()\n",
    "\n",
    "# Use extract year function to get the year\n",
    "data['contract'] = data['contract'].apply(extract_year)\n",
    "\n",
    "# Check what non-integer values remained in the column:\n",
    "data['contract'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values having `On Loan` at the end mean that the player still has a contract with the club, but he's borrowed to a different team in the meantime. We can create a separate column to track the players that have been on loan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_loans(x):\n",
    "    if type(x) == str and 'On Loan' in x:\n",
    "        record = 'Yes'\n",
    "    else:\n",
    "        record = 'No'\n",
    "    return record    \n",
    "\n",
    "data['on_loan'] = data['contract'].apply(record_loans)\n",
    "\n",
    "# Check the operation was successful\n",
    "data['on_loan'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now remove the 'On Loan' string from the contract column to extract the year:\n",
    "def clean_loans(x):\n",
    "    try:\n",
    "        x = int(x.replace(\" On Loan\", \"\")[-4:])\n",
    "    except:\n",
    "        pass\n",
    "    return x    \n",
    "\n",
    "data['contract'] = data['contract'].apply(clean_loans)\n",
    "\n",
    "# Check the operation was successful\n",
    "data['contract'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice the only odd values left are the ones looking like *Country Free* and the *1648*, so we will look at how many rows have these type of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd_ones_out = []\n",
    "\n",
    "for value in data['contract']:\n",
    "    if type(value) == str and 'Free' in value:\n",
    "        odd_ones_out.append(value)\n",
    "    elif value == 1648:\n",
    "        odd_ones_out.append(value)\n",
    "\n",
    "len(odd_ones_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are very few odd values compared to the number of rows in the dataset, we can replace them with nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_odds(x):\n",
    "    if x in odd_ones_out:\n",
    "        x = np.nan\n",
    "    return x    \n",
    "\n",
    "data['contract'] = data['contract'].apply(clean_odds)\n",
    "\n",
    "# Rename column to end_contract\n",
    "data.rename({'contract': 'end_contract'}, axis=1, inplace=True)\n",
    "\n",
    "# Check the operation was successful\n",
    "data['end_contract'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nationality` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nationality\"] = data[\"nationality\"].apply(lambda x: \"Congo\" if str(x).startswith(\"DR\")\n",
    "                                                 else \"North Korea\" if str(x).endswith(\"DPR\")\n",
    "                                                 else \"China\" if str(x).endswith(\"PR\")\n",
    "                                                 else str(x).replace(\"&amp;\",\"and\") if \"&amp;\" in x\n",
    "                                                 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also check if there is any numerical data still stored as an object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    print(col, \":\", data[col].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the `hits` column is still an object, we will change it to numerical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hits'] = pd.to_numeric(data['hits'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Find & replace null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check there aren't any rows with too many null values:\n",
    "def check_null_values_rows(df, threshold=75):\n",
    "    nulls_percentage = {}\n",
    "    for index in df.index:\n",
    "        number_of_nulls = df.loc[index,].isna().sum()\n",
    "        null_percentage = round(number_of_nulls * 100 / df.shape[1], 1)\n",
    "        if null_percentage >= threshold:\n",
    "            nulls_percentage[index] = null_percentage\n",
    "    return nulls_percentage\n",
    "\n",
    "check_null_values_rows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are no rows with more than 75% `NaN` values, we can begin replacing these values per column. Before doing so, however, we will also check the maximum amount of nulls we have in any given row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_nulls_rows(df):\n",
    "    nulls_percentage = []\n",
    "    for index in df.index:\n",
    "        number_of_nulls = df.loc[index,].isna().sum()\n",
    "        null_percentage = round(number_of_nulls * 100 / df.shape[1], 1)\n",
    "        nulls_percentage.append(null_percentage)\n",
    "    return max(nulls_percentage)\n",
    "\n",
    "max_nulls_rows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the maximum amount of null values within a row is only ~15%, we can move on to replacing the null values in the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of null values\n",
    "for col in data.columns:\n",
    "    if data[col].isna().sum() != 0:\n",
    "        print(col, \":\", round((data[col].isna().sum()) * 100 / data.shape[0], 0),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max left is 2% so we can safely drop the remaining NaN values:\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis\n",
    "\n",
    "#### 2.1. Categorical & Discrete Numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical and discrete numerical data\n",
    "cat_cols = []\n",
    "\n",
    "for column in data.columns:\n",
    "    if len(data[column].unique()) <= 10 or data[column].dtypes == np.object:\n",
    "        cat_cols.append(column)\n",
    "\n",
    "# Show bar plots for categorical data with few values\n",
    "for column in cat_cols:\n",
    "    x = data[column].unique()\n",
    "    y = data[column].value_counts()\n",
    "    if len(x) < 100:\n",
    "        fig, ax = plt.subplots(figsize = (12, 9))\n",
    "        plt.title(column)\n",
    "        plt.bar(x, y)\n",
    "        fig.savefig('{}.png'.format(column))\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots, we see that:\n",
    "* most players in the dataset are in center positions (*CM*, *CAM*, *CF*)\n",
    "- players' position rating is mostly from 1-3 stars\n",
    "* ~ 3/4 of players prefer playing with their right-foot\n",
    "- most players play relatively well with their weak foot (4-5 ratings)\n",
    "* most players have a 3-4 rating for skill moves\n",
    "- most players have a moderate amount of wins away from home, as well as draws\n",
    "* the vast majority of the players in the dataset have an international reputation of 1 star\n",
    "- the vast majority of players play for the team they've been contracted in, rather than on loan\n",
    "\n",
    "Therefore, we'll remove the `international_reputation` and `on_loan` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['international_reputation', 'on_loan'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given there are too many categories in the `nationality` and `club` columns, we cannot easily visualize them using (small) bar plots. However, inspired by [4m4n5's graph](https://github.com/4m4n5/fifa18-all-player-statistics) for representing nationality data, we used a treemap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality = pd.DataFrame(data['nationality'].value_counts())\n",
    "nationality.sort_values(ascending=False, inplace=True, by='nationality')\n",
    "nationality.reset_index(inplace=True)\n",
    "nationality.rename({'index': 'nationality', 'nationality':'number_of_players'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename countries with less than 10 players to 'Other' and remove columns:\n",
    "other_count = 0\n",
    "\n",
    "for i in range(0, nationality.shape[0]):\n",
    "    if nationality.loc[i, 'number_of_players'] < 30:\n",
    "        nationality.loc[i,'nationality'] = 'Other'\n",
    "        other_count += nationality.loc[i, 'number_of_players']\n",
    "        nationality.drop(i, axis=0, inplace=True)\n",
    "\n",
    "# Add back the consolidated 'Other' column:\n",
    "nationality.loc[len(nationality.index)] = ['Other', other_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "fig, ax = plt.subplots(figsize = (20, 9))\n",
    "squarify.plot(sizes=nationality['number_of_players'], label=nationality['nationality'], alpha=.8)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Continuous numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a look at the overall and potential scores of the players, to get an idea of whether or not they would be collinear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the best players by overall score\n",
    "best_players = data.sort_values([\"overall_score\"], ascending=[False])\n",
    "rank = best_players[[\"name\", \"overall_score\"]]\n",
    "rank.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the best potential players\n",
    "best_pot = data.sort_values([\"potential_score\"], ascending=[False])\n",
    "rank_2 = best_players[[\"name\", \"potential_score\"]]\n",
    "rank_2.head(10) # same players as sorted by overall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would there be any difference between players with the worst overall and potential score?\n",
    "worst_players = data.sort_values([\"overall_score\"], ascending=[False])\n",
    "rank = worst_players[[\"name\", \"overall_score\"]]\n",
    "rank.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_pot = data.sort_values([\"potential_score\"], ascending=[False])\n",
    "rank_2 = worst_players[[\"name\", \"potential_score\"]]\n",
    "rank_2.tail(10) # the players are exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many hits do the best players have?\n",
    "best_hits = data.sort_values([\"overall_score\", \"hits\"], ascending=[False, False])\n",
    "rank_3 = best_players[[\"name\", \"overall_score\", \"hits\"]]\n",
    "rank_3.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will have a look at the data distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the numerical data\n",
    "num = data.select_dtypes(np.number)\n",
    "\n",
    "# Extract the continuous numerical data and plot it\n",
    "for column in num.columns:\n",
    "    if len(data[column].unique()) > 10:\n",
    "        sns.displot(data[column])\n",
    "        plt.savefig('num_graphs/{}.png'.format(column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that many of the attributes are normally distributed, except for the `wage`, `value`, `release_clause`, `hits`, and `joined` columns, which are heavily skewed towards lower values. We'll also represent the data using boxplots, to get an image of the amount of outliers present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the continuous numerical data and plot it\n",
    "for column in num.columns:\n",
    "    if len(data[column].unique()) > 10:\n",
    "        sns.boxplot(x=column, data=data)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there will be a considerable amount of outliers in:\n",
    "- distributions that have two medians\n",
    "* non-Gaussian distributions, e.g. `value`, `wage`, `release_clause`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Correlation between different attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "data.corr()\n",
    "\n",
    "# Create heatmap\n",
    "mask = np.zeros_like(data.corr())\n",
    "mask[np.triu_indices_from(mask)] = True # optional, to hide repeat half of the matrix\n",
    "fig, ax = plt.subplots(figsize=(80, 76))\n",
    "ax = sns.heatmap(data.corr(), mask=mask, annot=True)\n",
    "plt.show()\n",
    "\n",
    "# first observations:\n",
    "# value, wage and release clause highly correlated to each other (around 0.6)\n",
    "# postions very highly correlated (>0.95) to stats that fit positions, ie. attacking positions corrlated to attacking stats\n",
    "  # => for the future rounds: \n",
    "    # positions and stats are highly correlated data:\n",
    "    # repetition of exogenous variables can twist the model, will probably be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that:\n",
    "- the `goalkeeper` column is highly correlated (> 0.9) with the `goalkeeping` and `gk_[trait]` columns\n",
    "* the `gk_[trait]` columns also seem to be highly correlated with each other\n",
    "\n",
    "Whilst these were easy to spot, there are many other features that are highly correlated between themselves. Therefore, we created a function to find them and remove the feature that is the least correlated with a chosen target, which we'll use when we begin modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_destroyer(data, target, max_threshold=0.95):\n",
    "    corr_data = data.corr()\n",
    "    corr_target = corr_data[target]\n",
    "    corr_data.drop(target, axis=1, inplace=True)\n",
    "    corr_data.drop(target, axis=0, inplace=True)\n",
    "    \n",
    "    column_no = corr_data.shape[0]\n",
    "    to_drop = []\n",
    "\n",
    "    for i in range(0, column_no):\n",
    "        for j in range(i + 1, column_no):\n",
    "            if corr_data.iloc[i, j] > max_threshold:\n",
    "                if corr_target.iloc[i] > corr_target.iloc[j]:\n",
    "                    to_drop.append(corr_data.columns[j])\n",
    "                else:                 \n",
    "                    to_drop.append(corr_data.columns[i])\n",
    "    \n",
    "    to_drop = list(set(to_drop)) # Get unique values\n",
    "    return to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Modelling & Evaluation\n",
    "\n",
    "#### 3.1. Define normalizing & modelling functions\n",
    "\n",
    "##### 3.1.1. Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df):\n",
    "    num = df.select_dtypes(np.number)\n",
    "    transformer = MinMaxScaler().fit(num) \n",
    "    x_minmax = transformer.transform(num)\n",
    "    num_norm = pd.DataFrame(x_minmax, columns=num.columns)\n",
    "    return num_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2. Box-Cox transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxcox_transform(data):\n",
    "    numeric_cols = data.select_dtypes(np.number).columns\n",
    "    _ci = {column: None for column in numeric_cols}\n",
    "    for column in numeric_cols:\n",
    "        # since i know any columns should take negative numbers, to avoid -inf in df\n",
    "        data[column] = np.where(data[column]<=0, np.NAN, data[column]) \n",
    "        data[column] = data[column].fillna(data[column].median())\n",
    "        transformed_data, ci = stats.boxcox(data[column])\n",
    "        data[column] = transformed_data\n",
    "        _ci[column] = [ci] \n",
    "    return data, _ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3. Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, threshold=1.5):\n",
    "    numerical = df.select_dtypes(np.number)\n",
    "    columns = numerical.columns\n",
    "    for column in columns:\n",
    "        if len(df[column].unique()) < 10:\n",
    "            continue\n",
    "        else:\n",
    "            upper = np.percentile(df[column], 75)\n",
    "            lower = np.percentile(df[column], 25)\n",
    "            iqr = upper - lower\n",
    "            upper_limit = upper + threshold * iqr\n",
    "            lower_limit = lower - threshold * iqr\n",
    "            df = df[(df[column]>lower_limit) & (df[column]<upper_limit)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.5. Encode categorical data (`get_dummies`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat(df):\n",
    "    cat = df.select_dtypes(np.object)\n",
    "    cat = pd.get_dummies(df, columns=df.columns, drop_first=True)\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.6. Concatenate numerical and categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_df(num, cat):\n",
    "    new_df = pd.concat([num, cat], axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.7. Running & evaluating the model\n",
    "\n",
    "Looking at the R2 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(df, target):\n",
    "    y = df[target]\n",
    "    X = df.drop(target, axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    return r2, predictions, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the predictions versus the real data and analyzing the MSE, RMSE, and MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reversing the Box-cox transformation\n",
    "def reversing(predictions, _ci, target):\n",
    "    predictions = inv_boxcox(predictions, _ci[target][0])\n",
    "    return predictions\n",
    "\n",
    "# Calculate the modified metrics\n",
    "def evaluate_model_2(y_test, predictions):\n",
    "    RMSE = mean_squared_error(y_test, predictions, squared=False)\n",
    "    MSE = mean_squared_error(y_test, predictions)\n",
    "    MAE = mean_absolute_error(y_test, predictions)\n",
    "    return print(\"RMSE =\", RMSE), print(\"MSE =\", MSE), print(\"MAE =\", MAE)\n",
    "\n",
    "# Create dataframe for visualising the differences between real and predicted values\n",
    "def diff_df(y_test, _ci, target, predictions):\n",
    "    results = pd.DataFrame()\n",
    "    results['true'] = inv_boxcox(y_test, _ci[target][0])\n",
    "    results['pred'] = predictions\n",
    "    results['diff'] = results.apply(lambda x: abs(x['true'] - x['pred']), axis=1)\n",
    "    results = results.sort_values('diff', ascending=False).head(20)\n",
    "    return results\n",
    "\n",
    "# Plot results for visual representation\n",
    "def we_like_to_see(results):\n",
    "    beautiful_graph = sns.regplot(results['true'], results['pred'])\n",
    "    return beautiful_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the R2 adjusted to check which features contribute to the R2 score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Optimize model for the `overall_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe copy\n",
    "data_1 = data.copy()\n",
    "\n",
    "# Remove correlated data\n",
    "to_drop = corr_destroyer(data_1, target='overall_score')\n",
    "data.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# 1. Remove outliers\n",
    "\n",
    "\n",
    "# 2. Box-Cox transform\n",
    "data_1, _ci_1 = boxcox_transform(data_1)\n",
    "\n",
    "# 3. Scale numerical data\n",
    "num_norm_1 = normalize_data(data_1)\n",
    "\n",
    "# 4. Encode categorical data\n",
    "cat_1 = encode_cat(data_1)\n",
    "\n",
    "# 5. Merge numerical & categorical data\n",
    "new_df_1 = new_df(num_norm_1, cat_1)\n",
    "\n",
    "# 6. Run regression\n",
    "r2_1, predictions_1, y_test_1 = regression(new_df_1)\n",
    "print(r2_1)\n",
    "\n",
    "# 7. See the predictions\n",
    "predictions_1 = reversed(predictions_1)\n",
    "evaluate_model_2(y_test_1, predictions_1)\n",
    "results = diff_df(y_test_1, _ci_1, target='overall_score', predictions=predictions_1)\n",
    "we_like_to_see(results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

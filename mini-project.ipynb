{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Review & Cleaning\n",
    "\n",
    "#### 1.1. Initial review - remove redundant columns, standardize column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "data = pd.read_csv(\"fifa21_male2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a quick look at the data:\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice immediately that the columns `Player Photo`, `Club Logo`, and `Flag Photo` contain *sofifa* links, which will not help in the current analysis, so we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Player Photo', 'Club Logo', 'Flag Photo'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice there is an ID column which we can use as our index column, after ensuring it doesn't have any duplicate values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data['ID'].unique())/data.shape[0]) # equal to 1, so no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('ID', inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also remove the `Name` column, as it's unlikely to be relevant to our analysis and store it in a different dataframe for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = data['Name']\n",
    "data.drop('Name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice there are many columns names which are abbreviations - so we will rename those to the full name for clarity. To do so, we are using a `.csv` file with the abbreviations and their corresponding meaning, as seen on the *sofifa* website. Alongside, we will also standardize the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change capitalization to lowercase and replace spaces with underscores:\n",
    "data.columns = data.columns.str.lower()\n",
    "data.columns = data.columns.str.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read positions.csv into dataframe\n",
    "positions = pd.read_csv('positions.csv', header=None, index_col=0)\n",
    "\n",
    "# Change dataframe to series and then dictionary so it can be used to rename columns:\n",
    "positions = positions.squeeze().to_dict()\n",
    "\n",
    "# Change column names:\n",
    "for column in data.columns:\n",
    "    if column in positions:\n",
    "        data.rename(columns=positions, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can quickly check which columns have more than 75% null values, so we can discard them from our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_values(df, threshold=75):\n",
    "    nulls_percentage = {}\n",
    "    for column in df.columns:\n",
    "        number_of_nulls = df[column].isna().sum()\n",
    "        null_percentage = round(number_of_nulls * 100 / df.shape[0], 1)\n",
    "        if null_percentage >= threshold:\n",
    "            nulls_percentage[column] = null_percentage\n",
    "    return nulls_percentage\n",
    "\n",
    "check_null_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As `loan_date_end` has mostly `NaN` values, we can discard it:\n",
    "data.drop('loan_date_end', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of the amount of null values in our data, we can check the maximum percentage of `NaN` values in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_nulls(df):\n",
    "    nulls_percentage = []\n",
    "    for column in df.columns:\n",
    "        number_of_nulls = df[column].isna().sum()\n",
    "        null_percentage = round(number_of_nulls * 100 / df.shape[0], 1)\n",
    "        nulls_percentage.append(null_percentage)\n",
    "    return max(nulls_percentage)\n",
    "\n",
    "max_nulls(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the maximum amount of nulls is low (2.5%), we can continue with our initial review and get back to replacing null values later. \n",
    "\n",
    "Next, we'll review the number of unique values per column and ensure there are no columns with one value only, as they will not add any value to the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_values(df):\n",
    "    single_value_columns = []\n",
    "    for column in df:\n",
    "        if len(df[column].unique()) == 1:\n",
    "            single_value_columns.append(column)\n",
    "    return single_value_columns\n",
    "\n",
    "check_unique_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values in the gender column\n",
    "data['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the gender column as the data shows only male players\n",
    "data.drop('gender', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking once more at the data, we observe that the `team_&_contract` column seems to have similar information to the `club` & `contract` columns, so we might be able to remove it too. However, we first have to check our assumption is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a joined column to compare to the team & contract column\n",
    "for i in range(0, data.shape[0]):\n",
    "    if data['contract'].iloc[i] == np.nan:\n",
    "        data['contract'].iloc[i] = ' '\n",
    "    if data['club'].iloc[i] == np.nan:\n",
    "        data['club'].iloc[i] = ' '\n",
    "\n",
    "data['club_&_contract'] = data['club'] + ' ' + data['contract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the columns are identical and save non-identical values to a dataframe\n",
    "def check_identical_columns(col1, col2, df):\n",
    "    diff_values = pd.DataFrame({col1:[], col2:[]})\n",
    "    identical = 1\n",
    "\n",
    "    for i in range(0, df.shape[0]):\n",
    "        if df[col1].iloc[i] == df[col2].iloc[i]:\n",
    "            continue\n",
    "        else:\n",
    "            diff_values.loc[len(diff_values.index)] = [df[col1].iloc[i], df[col2].iloc[i]]\n",
    "            identical = 0\n",
    "            \n",
    "\n",
    "    if identical == 0:\n",
    "        return diff_values\n",
    "    else:\n",
    "        return 'Columns are identical.'         \n",
    "            \n",
    "diff_values = check_identical_columns(col1='club_&_contract', col2='team_&_contract', df=data)        \n",
    "\n",
    "diff_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice two things:\n",
    "- the `club_&_contract` column has null values, whereas the `team_&_contract` column doesn't\n",
    "* the `club_&_contract` rows that are non-identical also contain the country that the club belongs to\n",
    "\n",
    "As the number of values containing the country are low compared to the number of rows in the dataframe, we'll quickly check if the null values come from the `club` and / or `contract` column to see if we can discard the `team_&_contract` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the rows with null values\n",
    "diff_values[diff_values['club_&_contract'].isna()]\n",
    "\n",
    "# Check if the nulls come from the contract or club columns:\n",
    "print(data['contract'].isna().sum())\n",
    "print(data['club'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the null values in the `club_&_contract` column came from the `club` column. However, given that the `team_&_contract` values corresponding to those rows also contain only the contract information, we can remove it, as it doesn't provide any additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['team_&_contract', 'club_&_contract'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Data Cleaning\n",
    "\n",
    "##### 1.2.1. Numerical Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`value`, `wage`, and `release clause` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financials = ['value', 'wage', 'release_clause']\n",
    "\n",
    "def clean_value(i):\n",
    "    x = float(i.replace(\".\",\"\").replace(\"€\",\"\").replace(\"K\",\"000\").replace(\"M\",\"00000\"))\n",
    "    return x\n",
    "\n",
    "for column in financials:\n",
    "    data[column] = data[column].apply(clean_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weight` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_weight(i):\n",
    "    x = float(i.replace('lbs',''))\n",
    "    return x\n",
    "\n",
    "data[\"weight\"] = data[\"weight\"].apply(clean_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`height` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_height(i):\n",
    "    to_cm = 2.54\n",
    "    x = i.replace(\"'\",\" \")\n",
    "    x2 = x.replace('\"','') # need to do it in two parts \n",
    "                           # because of different quote used \n",
    "                           # for inch and foot \n",
    "    y = x2.split()\n",
    "    height = round(((float(y[0])*12)+float(y[1]))*to_cm,0)\n",
    "    return height\n",
    "\n",
    "data['height'] = data['height'].apply(convert_height)\n",
    "data['height']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the positions columns, i.e. `left-striker`, `goalkeeper`, etc. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_positions(i):\n",
    "    x = float(i.replace(\"+\",\".\").replace(\"-\",\"\"))\n",
    "    return x\n",
    "\n",
    "for col in data.loc[:, 'left_striker':'goalkeeper']:\n",
    "    data[col] = data[col].apply(cleaning_positions)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to cleaning the data within, we will also find which columns have identical scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll first extract the names of the positions that have players on the right / left / center\n",
    "first_position = data.columns.get_loc('left_striker') \n",
    "last_position = data.columns.get_loc('goalkeeper')\n",
    "\n",
    "position_names = []\n",
    "\n",
    "for i in range(first_position, last_position + 1):\n",
    "    column_name = data.columns[i]\n",
    "    if 'left' in column_name:\n",
    "        position_names.append(column_name[5:])\n",
    "\n",
    "print(position_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll find the groups that we'd like to check\n",
    "groups = {}\n",
    "\n",
    "for position in position_names:\n",
    "    placements = []\n",
    "    for i in range(first_position, last_position + 1):\n",
    "        column_name = data.columns[i]\n",
    "        if position in column_name:\n",
    "            placements.append(column_name)\n",
    "    groups[position] = placements\n",
    "\n",
    "# Remove the extra values from the 'back' list:\n",
    "groups['back'] = ['left_back', 'center_back', 'right_back']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find identical columns and store the indexes in a list to later drop\n",
    "to_drop = []\n",
    "\n",
    "for position in groups.keys():\n",
    "    positions_number = len(groups[position])\n",
    "    for i in range(0, positions_number):\n",
    "        for j in range(i + 1, positions_number):\n",
    "            column_1 = groups[position][i]\n",
    "            column_2 = groups[position][j]\n",
    "            if data[column_1].equals(data[column_2]):\n",
    "                if column_2 not in to_drop:\n",
    "                    to_drop.append(column_2)\n",
    "\n",
    "# Drop redundant columns\n",
    "data.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'left_' string from the remaining columns\n",
    "for column in data.columns:\n",
    "    if 'left' in column:\n",
    "        data.rename({column: column[5:]}, axis=1, inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weak_foot`, `skill_moves`, and `international_reputation` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_columns = ['weak_foot', 'skill_moves', 'international_reputation']\n",
    "\n",
    "# Check unique values\n",
    "for column in star_columns:\n",
    "    print(data[column].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the first character from the string, then convert the data type to integer\n",
    "# and check the operation was successful\n",
    "for column in star_columns:\n",
    "    data[column] = data[column].str[0]\n",
    "    data[column] = pd.to_numeric(data[column], errors='raise')\n",
    "    print(data[column].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2. Categorical Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the `positions` data to the number of possible positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_positions(x):\n",
    "    number_of_positions = str(x).split() \n",
    "    position = len(number_of_positions) \n",
    "    return position \n",
    "\n",
    "data['position'] = data['position'].apply(clean_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the year from the `joined` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the end of the joined is typically represented by the last 4 characters of the \n",
    "# strings, we will extract those where possible:\n",
    "def extract_year(x):\n",
    "    try:\n",
    "        x = int(x[-4:])\n",
    "    except:\n",
    "        pass\n",
    "    return x\n",
    "\n",
    "data['joined'] = data['joined'].apply(extract_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same function to extract the end year from the `contract` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the contract data\n",
    "data['contract'].unique()\n",
    "\n",
    "# Use extract year function to get the year\n",
    "data['contract'] = data['contract'].apply(extract_year)\n",
    "\n",
    "# Check what non-integer values remained in the column:\n",
    "data['contract'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values having `On Loan` at the end mean that the player still has a contract with the club, but he's borrowed to a different team in the meantime. We can create a separate column to track the players that have been on loan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_loans(x):\n",
    "    if type(x) == str and 'On Loan' in x:\n",
    "        record = 'Yes'\n",
    "    else:\n",
    "        record = 'No'\n",
    "    return record    \n",
    "\n",
    "data['on_loan'] = data['contract'].apply(record_loans)\n",
    "\n",
    "# Check the operation was successful\n",
    "data['on_loan'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now remove the 'On Loan' string from the contract column to extract the year:\n",
    "def clean_loans(x):\n",
    "    try:\n",
    "        x = int(x.replace(\" On Loan\", \"\")[-4:])\n",
    "    except:\n",
    "        pass\n",
    "    return x    \n",
    "\n",
    "data['contract'] = data['contract'].apply(clean_loans)\n",
    "\n",
    "# Check the operation was successful\n",
    "data['contract'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice the only odd values left are the ones looking like *Country Free* and the *1648*, so we will look at how many rows have these type of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd_ones_out = []\n",
    "\n",
    "for value in data['contract']:\n",
    "    if type(value) == str and 'Free' in value:\n",
    "        odd_ones_out.append(value)\n",
    "    elif value == 1648:\n",
    "        odd_ones_out.append(value)\n",
    "\n",
    "len(odd_ones_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are very few odd values compared to the number of rows in the dataset, we can replace them with nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_odds(x):\n",
    "    if x in odd_ones_out:\n",
    "        x = np.nan\n",
    "    return x    \n",
    "\n",
    "data['contract'] = data['contract'].apply(clean_odds)\n",
    "\n",
    "# Rename column to end_contract\n",
    "data.rename({'contract': 'end_contract'}, axis=1, inplace=True)\n",
    "\n",
    "# Check the operation was successful\n",
    "data['end_contract'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nationality` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nationality\"] = data[\"nationality\"].apply(lambda x: \"Democratic Republic of the Congo\" if str(x).startswith(\"DR\")\n",
    "                                                 else \"North Korea\" if str(x).endswith(\"DPR\")\n",
    "                                                 else \"China\" if str(x).endswith(\"PR\")\n",
    "                                                 else str(x).replace(\"&amp;\",\"and\") if \"&amp;\" in x\n",
    "                                                 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also check if there is any numerical data still stored as an object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    print(col, \":\", data[col].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the `hits` column is still an object, we will change it to numerical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hits'] = pd.to_numeric(data['hits'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Find & replace null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check there aren't any rows with too many null values:\n",
    "def check_null_values_rows(df, threshold=75):\n",
    "    nulls_percentage = {}\n",
    "    for index in df.index:\n",
    "        number_of_nulls = df.loc[index,].isna().sum()\n",
    "        null_percentage = round(number_of_nulls * 100 / df.shape[1], 1)\n",
    "        if null_percentage >= threshold:\n",
    "            nulls_percentage[index] = null_percentage\n",
    "    return nulls_percentage\n",
    "\n",
    "check_null_values_rows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are no rows with more than 75% `NaN` values, we can begin replacing these values per column. Before doing so, however, we will also check the maximum amount of nulls we have in any given row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_nulls_rows(df):\n",
    "    nulls_percentage = []\n",
    "    for index in df.index:\n",
    "        number_of_nulls = df.loc[index,].isna().sum()\n",
    "        null_percentage = round(number_of_nulls * 100 / df.shape[1], 1)\n",
    "        nulls_percentage.append(null_percentage)\n",
    "    return max(nulls_percentage)\n",
    "\n",
    "max_nulls_rows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the maximum amount of null values within a row is only ~15%, we can move on to replacing the null values in the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of null values\n",
    "for col in data.columns:\n",
    "    if data[col].isna().sum() != 0:\n",
    "        print(col, \":\", round((data[col].isna().sum()) * 100 / data.shape[0], 0),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max left is 2% so we can safely drop the remaining NaN values:\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Exploratory Data Analysis\n",
    "\n",
    "##### 1.4.1. Categorical & Discrete Numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical data\n",
    "cat = data.select_dtypes(np.object)\n",
    "\n",
    "# Show bar plots for categorical data with few values\n",
    "for column in cat:\n",
    "    x = cat[column].unique()\n",
    "    y = cat[column].value_counts()\n",
    "    if len(x) < 100:\n",
    "        fig, ax = plt.subplots(figsize = (12, 9))\n",
    "        plt.title(column)\n",
    "        plt.bar(x, y)\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

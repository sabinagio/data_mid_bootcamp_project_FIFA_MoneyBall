{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Review & Cleaning\n",
    "\n",
    "#### 1.1. Initial review - remove redundant columns, standardize column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "data = pd.read_csv(\"fifa21_male2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a quick look at the data:\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice immediately that the columns `Player Photo`, `Club Logo`, and `Flag Photo` contain *sofifa* links, which will not help in the current analysis, so we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Player Photo', 'Club Logo', 'Flag Photo'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice there is an ID column which we can use as our index column, after ensuring it doesn't have any duplicate values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the number of unique values in the ID column are the same as the number of rows\n",
    "# in the dataframe, we can set it as the index.\n",
    "print(len(data['ID'].unique())/data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('ID', inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice that there are many columns which contain abbreviations that are unfamiliar for a non-player, so we will rename the columns to include the full names of the features. We will do so by using a `.csv` file containing the values for the abbreviations as seen on the *sofifa* website and standardize the column names alongside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change capitalization to lowercase and replace spaces with underscores:\n",
    "data.columns = data.columns.str.lower()\n",
    "data.columns = data.columns.str.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read positions.csv into dataframe\n",
    "positions = pd.read_csv('positions.csv', header=None, index_col=0)\n",
    "\n",
    "# Change dataframe to series and then dictionary so it can be used to rename columns:\n",
    "positions = positions.squeeze().to_dict()\n",
    "\n",
    "# Change the column names:\n",
    "for column in data:\n",
    "    if column in positions:\n",
    "        data.rename(columns=positions, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly check how many columns have more than 75% of null values, so we can discard them from our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_values(df, threshold=75):\n",
    "    nulls_percentage = {}\n",
    "    for column in df:\n",
    "        number_of_nulls = df[column].isna().sum()\n",
    "        null_percentage = round(number_of_nulls * 100 / df.shape[0], 1)\n",
    "        if null_percentage >= threshold:\n",
    "            nulls_percentage[column] = null_percentage\n",
    "    return nulls_percentage\n",
    "\n",
    "check_null_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As `loan_date_end` has mostly `NaN` values, we can discard it:\n",
    "data.drop('loan_date_end', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a quick look at the maximum percentage of `NaN` values in any column for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_nulls(df):\n",
    "    nulls_percentage = []\n",
    "    for column in df:\n",
    "        number_of_nulls = df[column].isna().sum()\n",
    "        null_percentage = round(number_of_nulls * 100 / df.shape[0], 1)\n",
    "        nulls_percentage.append(null_percentage)\n",
    "    return max(nulls_percentage)\n",
    "\n",
    "max_nulls(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the maximum amount of nulls in any column is 2.5%, we can continue with our initial review of the data. We can have a look at the number of unique values per column to see if there are any columns that have only one value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_values(df):\n",
    "    single_value_columns = []\n",
    "    for column in df:\n",
    "        if len(df[column].unique()) == 1:\n",
    "            single_value_columns.append(column)\n",
    "    return single_value_columns\n",
    "\n",
    "check_unique_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values in the gender column\n",
    "data['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Gender column as the data shows only male players\n",
    "data.drop('gender', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the `team_&_contract` column seems to have the same information as the `club` & `contract` columns, so we might be able to remove it as well after checking that our assumption is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a joined column to compare to the team & contract column\n",
    "data['club_&_contract'] = data['club'] + ' ' + data['contract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['club_&_contract'].head())\n",
    "print(data['team_&_contract'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the columns are identical\n",
    "def check_identical_columns(col1, col2, df):\n",
    "    diff_values = pd.DataFrame({col1:[], col2:[]})\n",
    "    identical = 1\n",
    "\n",
    "    for i in range(0, df.shape[0]):\n",
    "        if df[col1].iloc[i] == df[col2].iloc[i]:\n",
    "            continue\n",
    "        else:\n",
    "            diff_values.loc[len(diff_values.index)] = [df[col1].iloc[i], df[col2].iloc[i]]\n",
    "            identical = 0\n",
    "            \n",
    "\n",
    "    if identical == 0:\n",
    "        print(diff_values)\n",
    "    else:\n",
    "        return 'Columns are identical.'         \n",
    "            \n",
    "check_identical_columns(col1='club_&_contract', col2='team_&_contract', df=data)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values in columns:\n",
    "print(data['club_&_contract'].isna().sum())\n",
    "print(data['team_&_contract'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if columns are identical after removing the rows with null values:\n",
    "data_test = data.copy()\n",
    "data_test.dropna(axis=0, how=\"any\", subset=['club_&_contract'], inplace=True)\n",
    "\n",
    "\n",
    "check_identical_columns(col1='club_&_contract', col2='team_&_contract', df=data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can remove Team & Contract column, given the same information is present in the Club\n",
    "# and Contract columns\n",
    "#data.drop('Team & Contract', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Data Cleaning\n",
    "\n",
    "##### 1.2.1. Numerical Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`value`, `wage`, and `release clause` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financials = ['value', 'wage', 'release_clause']\n",
    "\n",
    "def clean_value(i):\n",
    "    x = float(i.replace(\".\",\"\").replace(\"â‚¬\",\"\").replace(\"K\",\"000\").replace(\"M\",\"00000\"))\n",
    "    return x\n",
    "\n",
    "for column in financials:\n",
    "    data[column] = data[column].apply(clean_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weight` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_weight(i):\n",
    "    x = float(i.replace('lbs',''))\n",
    "    return x\n",
    "\n",
    "data[\"weight\"] = data[\"weight\"].apply(clean_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`height` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_height(i):\n",
    "    to_cm = 2.54\n",
    "    x = i.replace(\"'\",\" \")\n",
    "    x2 = x.replace('\"','') # need to do it in two parts \n",
    "                           # because of different quote used \n",
    "                           # for inch and foot \n",
    "    y = x2.split()\n",
    "    height = round(((float(y[0])*12)+float(y[1]))*to_cm,0)\n",
    "    return height\n",
    "\n",
    "data['height'] = data['height'].apply(convert_height)\n",
    "data['height']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the positions columns, i.e. `left-striker`, `goalkeeper`, etc. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_positions(i):\n",
    "    x = float(i.replace(\"+\",\".\").replace(\"-\",\"\"))\n",
    "    return x\n",
    "\n",
    "for col in data.loc[:, 'left_striker':'goalkeeper']:\n",
    "    data[col] = data[col].apply(cleaning_positions)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weak_foot`, `skill_moves`, and `international_reputation` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_columns = ['weak_foot', 'skill_moves', 'international_reputation']\n",
    "\n",
    "# Check unique values\n",
    "for column in star_columns:\n",
    "    print(data[column].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the first character from the string, then convert the data type to integer\n",
    "# and check that the operation was successful\n",
    "for column in star_columns:\n",
    "    data[column] = data[column].str[0]\n",
    "    data[column] = pd.to_numeric(data[column], errors='raise')\n",
    "    print(data[column].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all edited "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2. Categorical Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the contract ending data from the `contract` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the contract data\n",
    "data['contract'].unique()\n",
    "\n",
    "# As the end of the contract is typically represented by the last 4 characters of the \n",
    "# strings, we will extract those where possible:\n",
    "def clean_contract(x):\n",
    "    try:\n",
    "        x = int(x[-4:])\n",
    "    except:\n",
    "        pass\n",
    "    return x\n",
    "\n",
    "data['contract'] = data['contract'].apply(clean_contract)\n",
    "\n",
    "# Check what non-integer values remained in the column:\n",
    "data['contract'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As there are many values ending with 'On Loan', we'll first check that these are not\n",
    "# equivalent to the joined date by looking at some data samples. Additionally,\n",
    "# we also want to see if the \"Country Free\" is related to the nationality of the player:\n",
    "print(data.loc[:, ['contract', 'joined', 'nationality']].head(50))\n",
    "print(data.loc[:, ['contract', 'joined', 'nationality']].tail(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now remove the 'On Loan' string from the contract column to extract the year and\n",
    "# replace all 'Country Free' values with NaN:\n",
    "def clean_loans(x):\n",
    "    try:\n",
    "        x = int(x.replace(\" On Loan\", \"\")[-4:])\n",
    "    except:\n",
    "        x = np.nan\n",
    "    return x    \n",
    "\n",
    "data['contract'] = data['contract'].apply(clean_loans)\n",
    "\n",
    "# Check that the operation was successful\n",
    "data['contract'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nationality` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nationality\"] = data[\"nationality\"].apply(lambda x: \"Democratic Republic of the Congo\" if str(x).startswith(\"DR\")\n",
    "                                                 else \"North Korea\" if str(x).endswith(\"DPR\")\n",
    "                                                 else \"China\" if str(x).endswith(\"PR\")\n",
    "                                                 else str(x).replace(\"&amp;\",\"and\") if \"&amp;\" in x\n",
    "                                                 else x)\n",
    "# data['nationality'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

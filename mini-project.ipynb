{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Review & Cleaning\n",
    "\n",
    "#### 1.1. Initial review - remove redundant columns, standardize column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "data = pd.read_csv(\"fifa21_male2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a quick look at the data:\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice immediately that the columns `Player Photo`, `Club Logo`, and `Flag Photo` contain *sofifa* links, which will not help in the current analysis, so we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Player Photo', 'Club Logo', 'Flag Photo'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice there is an ID column which we can use as our index column, after ensuring it doesn't have any duplicate values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data['ID'].unique())/data.shape[0]) # equal to 1, so no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('ID', inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice there are many columns names which are abbreviations - so we will rename those to the full name for clarity. To do so, we are using a `.csv` file with the abbreviations and their corresponding meaning, as seen on the *sofifa* website. Alongside, we will also standardize the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change capitalization to lowercase and replace spaces with underscores:\n",
    "data.columns = data.columns.str.lower()\n",
    "data.columns = data.columns.str.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read positions.csv into dataframe\n",
    "positions = pd.read_csv('positions.csv', header=None, index_col=0)\n",
    "\n",
    "# Change dataframe to series and then dictionary so it can be used to rename columns:\n",
    "positions = positions.squeeze().to_dict()\n",
    "\n",
    "# Change column names:\n",
    "for column in data:\n",
    "    if column in positions:\n",
    "        data.rename(columns=positions, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can quickly check which columns have more than 75% null values, so we can discard them from our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_values(df, threshold=75):\n",
    "    nulls_percentage = {}\n",
    "    for column in df:\n",
    "        number_of_nulls = df[column].isna().sum()\n",
    "        null_percentage = round(number_of_nulls * 100 / df.shape[0], 1)\n",
    "        if null_percentage >= threshold:\n",
    "            nulls_percentage[column] = null_percentage\n",
    "    return nulls_percentage\n",
    "\n",
    "check_null_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As `loan_date_end` has mostly `NaN` values, we can discard it:\n",
    "data.drop('loan_date_end', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of the amount of null values in our data, we can check the maximum percentage of `NaN` values in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_nulls(df):\n",
    "    nulls_percentage = []\n",
    "    for column in df:\n",
    "        number_of_nulls = df[column].isna().sum()\n",
    "        null_percentage = round(number_of_nulls * 100 / df.shape[0], 1)\n",
    "        nulls_percentage.append(null_percentage)\n",
    "    return max(nulls_percentage)\n",
    "\n",
    "max_nulls(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the maximum amount of nulls is low (2.5%), we can continue with our initial review and get back to replacing null values later. \n",
    "\n",
    "Next, we'll review the number of unique values per column and ensure there are no columns with one value only, as they will not add any value to the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_values(df):\n",
    "    single_value_columns = []\n",
    "    for column in df:\n",
    "        if len(df[column].unique()) == 1:\n",
    "            single_value_columns.append(column)\n",
    "    return single_value_columns\n",
    "\n",
    "check_unique_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values in the gender column\n",
    "data['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the gender column as the data shows only male players\n",
    "data.drop('gender', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking once more at the data, we observe that the `team_&_contract` column seems to have similar information to the `club` & `contract` columns, so we might be able to remove it too. However, we first have to check our assumption is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a joined column to compare to the team & contract column\n",
    "for i in range(0, data.shape[0]):\n",
    "    if data['contract'].iloc[i] == np.nan:\n",
    "        data['contract'].iloc[i] = ' '\n",
    "    if data['club'].iloc[i] == np.nan:\n",
    "        data['club'].iloc[i] = ' '\n",
    "\n",
    "data['club_&_contract'] = data['club'] + ' ' + data['contract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the columns are identical and save non-identical values to a dataframe\n",
    "def check_identical_columns(col1, col2, df):\n",
    "    diff_values = pd.DataFrame({col1:[], col2:[]})\n",
    "    identical = 1\n",
    "\n",
    "    for i in range(0, df.shape[0]):\n",
    "        if df[col1].iloc[i] == df[col2].iloc[i]:\n",
    "            continue\n",
    "        else:\n",
    "            diff_values.loc[len(diff_values.index)] = [df[col1].iloc[i], df[col2].iloc[i]]\n",
    "            identical = 0\n",
    "            \n",
    "\n",
    "    if identical == 0:\n",
    "        return diff_values\n",
    "    else:\n",
    "        return 'Columns are identical.'         \n",
    "            \n",
    "diff_values = check_identical_columns(col1='club_&_contract', col2='team_&_contract', df=data)        \n",
    "\n",
    "diff_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice two things:\n",
    "- the `club_&_contract` column has null values, whereas the `team_&_contract` column doesn't\n",
    "* the `club_&_contract` rows that are non-identical also contain the country that the club belongs to\n",
    "\n",
    "As the number of values containing the country are low compared to the number of rows in the dataframe, we'll quickly check if the null values come from the `club` and / or `contract` column to see if we can discard the `team_&_contract` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the rows with null values\n",
    "diff_values[diff_values['club_&_contract'].isna()]\n",
    "\n",
    "# Check if the nulls come from the contract or club columns:\n",
    "print(data['contract'].isna().sum())\n",
    "print(data['club'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the null values in the `club_&_contract` column came from the `club` column. However, given that the `team_&_contract` values corresponding to those rows also contain only the contract information, we can remove it, as it doesn't provide any additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['team_&_contract', 'club_&_contract'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Data Cleaning\n",
    "\n",
    "##### 1.2.1. Numerical Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`value`, `wage`, and `release clause` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financials = ['value', 'wage', 'release_clause']\n",
    "\n",
    "def clean_value(i):\n",
    "    x = float(i.replace(\".\",\"\").replace(\"â‚¬\",\"\").replace(\"K\",\"000\").replace(\"M\",\"00000\"))\n",
    "    return x\n",
    "\n",
    "for column in financials:\n",
    "    data[column] = data[column].apply(clean_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weight` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_weight(i):\n",
    "    x = float(i.replace('lbs',''))\n",
    "    return x\n",
    "\n",
    "data[\"weight\"] = data[\"weight\"].apply(clean_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`height` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_height(i):\n",
    "    to_cm = 2.54\n",
    "    x = i.replace(\"'\",\" \")\n",
    "    x2 = x.replace('\"','') # need to do it in two parts \n",
    "                           # because of different quote used \n",
    "                           # for inch and foot \n",
    "    y = x2.split()\n",
    "    height = round(((float(y[0])*12)+float(y[1]))*to_cm,0)\n",
    "    return height\n",
    "\n",
    "data['height'] = data['height'].apply(convert_height)\n",
    "data['height']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the positions columns, i.e. `left-striker`, `goalkeeper`, etc. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_positions(i):\n",
    "    x = float(i.replace(\"+\",\".\").replace(\"-\",\"\"))\n",
    "    return x\n",
    "\n",
    "for col in data.loc[:, 'left_striker':'goalkeeper']:\n",
    "    data[col] = data[col].apply(cleaning_positions)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weak_foot`, `skill_moves`, and `international_reputation` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_columns = ['weak_foot', 'skill_moves', 'international_reputation']\n",
    "\n",
    "# Check unique values\n",
    "for column in star_columns:\n",
    "    print(data[column].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the first character from the string, then convert the data type to integer\n",
    "# and check the operation was successful\n",
    "for column in star_columns:\n",
    "    data[column] = data[column].str[0]\n",
    "    data[column] = pd.to_numeric(data[column], errors='raise')\n",
    "    print(data[column].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2. Categorical Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the contract ending data from the `contract` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the contract data\n",
    "data['contract'].unique()\n",
    "\n",
    "# As the end of the contract is typically represented by the last 4 characters of the \n",
    "# strings, we will extract those where possible:\n",
    "def clean_contract(x):\n",
    "    try:\n",
    "        x = int(x[-4:])\n",
    "    except:\n",
    "        pass\n",
    "    return x\n",
    "\n",
    "data['contract'] = data['contract'].apply(clean_contract)\n",
    "\n",
    "# Check what non-integer values remained in the column:\n",
    "data['contract'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values having `On Loan` at the end mean that the player still has a contract with the club, but he's borrowed to a different team in the meantime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check similarities between columns\n",
    "print(data.loc[:, ['contract', 'joined', 'nationality']].head(50))\n",
    "print(data.loc[:, ['contract', 'joined', 'nationality']].tail(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now remove the 'On Loan' string from the contract column to extract the year and\n",
    "# replace all 'Country Free' values with NaN:\n",
    "def clean_loans(x):\n",
    "    try:\n",
    "        x = int(x.replace(\" On Loan\", \"\")[-4:])\n",
    "    except:\n",
    "        x = np.nan\n",
    "    return x    \n",
    "\n",
    "data['contract'] = data['contract'].apply(clean_loans)\n",
    "\n",
    "# Check the operation was successful\n",
    "data['contract'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nationality` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nationality\"] = data[\"nationality\"].apply(lambda x: \"Democratic Republic of the Congo\" if str(x).startswith(\"DR\")\n",
    "                                                 else \"North Korea\" if str(x).endswith(\"DPR\")\n",
    "                                                 else \"China\" if str(x).endswith(\"PR\")\n",
    "                                                 else str(x).replace(\"&amp;\",\"and\") if \"&amp;\" in x\n",
    "                                                 else x)\n",
    "# data['nationality'].unique()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
